{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch \n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- H1: DATASET UTILS -------------\n",
    "\n",
    "# ------- H2: Preprocess Data -------------\n",
    "def window_scale_divison(df, W, T, company_to_id, ticker):\n",
    "    \"\"\"\n",
    "        Returns the window of input and target values scaled by dividing with the\n",
    "        last value of the previous window.\n",
    "\n",
    "        Problems: With large W and T\n",
    "    \"\"\"\n",
    "    SMOOTH = 0.00001\n",
    "    list_df = [(\n",
    "                    (df['Open'][i+1:i+W+1] / df['Open'][i:i+1].values+SMOOTH).values, \n",
    "                    (df['High'][i+1:i+W+1] / df['High'][i:i+1].values+SMOOTH).values,           \n",
    "                    (df['Low'][i+1:i+W+1] / df['Low'][i:i+1].values+SMOOTH).values, \n",
    "                    (df['Close'][i+1:i+W+1] / df['Close'][i:i+1].values+SMOOTH).values,           \n",
    "                    (df['Volume'][i+1:i+W+1] / (df['Volume'][i:i+1].values+SMOOTH)).values, \n",
    "                    company_to_id[ticker],  \n",
    "                    df[i+1:i+W+1]['Date'], \n",
    "                    [\n",
    "                        (df['Close'][i+W+T:i+W+T+1] / df['Close'][i+W:i+W+1].values).values\n",
    "                        for T in [1, 5, 20]\n",
    "                    ], \n",
    "                    df['Close'][i+W:i+W+1],\n",
    "                    df.iloc[i+W:i+W+1, 7:].values,\n",
    "                    [\n",
    "                        (df['Low'][i+W+T:i+W+T+1]).values\n",
    "                        for T in [0, 1, 5, 20]\n",
    "                    ], \n",
    "                    [\n",
    "                        (df['High'][i+W+T:i+W+T+1]).values\n",
    "                        for T in [0, 1, 5, 20]\n",
    "                    ]\n",
    "                ) \n",
    "                for i in range(df.shape[0]-W-T)\n",
    "            ]\n",
    "    return list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- H2: Create Data -------------\n",
    "def create_batch_dataset(INDEX, W, T=20, problem='value', fast = False):\n",
    "    \n",
    "    #Created from where?\n",
    "    \n",
    "    directory = \"/data6/kpnaveen/Phase-Stock-KG/data/\" + \"nasdaq_2\" + \"/\"\n",
    "\n",
    "    company_to_id = {}\n",
    "    company_id    = 0\n",
    "\n",
    "    dataset = []\n",
    "    df_map = {}\n",
    "    skipped_ticker = []\n",
    "    total = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        if os.path.isfile(f):\n",
    "            print(filename)\n",
    "            ticker, name = filename.split(\"-\")\n",
    "            df = pd.read_csv(f)\n",
    "\n",
    "            # df = df.dropna()\n",
    "            df = df.fillna(method='ffill')\n",
    "\n",
    "            if df.shape[0] <= 2800:    # 13 years\n",
    "                print(\"Skipping file: Less Training-Testing samples [{0} samples]\".format(df.shape[0]))\n",
    "                skipped_ticker.append(ticker)\n",
    "                continue\n",
    "            total += 1\n",
    "\n",
    "            if ticker not in company_to_id:\n",
    "                company_to_id[ticker] = company_id\n",
    "                company_id += 1\n",
    "            \n",
    "            \n",
    "            if df.shape[0] > 2800:\n",
    "                df = df.iloc[-2800:]\n",
    "        \n",
    "            \"\"\"\n",
    "            annual_df = pd.read_csv(\"kg/fundamentals/macrotrends/combined-preprocessed/\"+ticker+\"-annual.csv\")\n",
    "            quarterly_df = pd.read_csv(\"kg/fundamentals/macrotrends/combined-preprocessed/\"+ticker+\"-quarterly.csv\")\n",
    "\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            annual_df['Date'] = pd.to_datetime(annual_df['Date'])\n",
    "            quarterly_df['Date'] = pd.to_datetime(quarterly_df['Date'])\n",
    "\n",
    "            # sort annual_df based on date\n",
    "            annual_df = annual_df.sort_values('Date')\n",
    "            quarterly_df = quarterly_df.sort_values('Date')\n",
    "            \n",
    "\n",
    "            # Join df on date with value greater than given\n",
    "            df = pd.merge_asof(df, annual_df, on='Date', direction='backward')\n",
    "            df = pd.merge_asof(df, quarterly_df, on='Date', direction='backward')\n",
    "\n",
    "            # Fill NaN values with previous values\n",
    "            df = df.fillna(method='ffill')\n",
    "            print(df.shape)\n",
    "            \"\"\"\n",
    "\n",
    "            list_df = [(\n",
    "                    df.iloc[i+W:i+W+1, 7:].shape\n",
    "                ) \n",
    "                for i in range(df.shape[0]-W-T)\n",
    "            ]\n",
    "\n",
    "            list_df = window_scale_divison(df, W, T, company_to_id, ticker)\n",
    "\n",
    "            df_map[company_to_id[ticker]] = list_df\n",
    "    \n",
    "    \n",
    "    #temporal_kg.pkl and temporal_kg_nifty.pkl created at kg/create_kg.ipynb and kg/create_kg_nifty.ipynb\n",
    "    kg_file_name = '/data6/kpnaveen/Phase-Stock-KG/kg/tkg_create/temporal_kg.pkl'\n",
    "    if INDEX == 'nifty500':\n",
    "        kg_file_name = '/data6/kpnaveen/Phase-Stock-KG/kg/tkg_create/temporal_kg_nifty.pkl'\n",
    "    with open(kg_file_name, 'rb') as f:\n",
    "        pkl_file = pickle.load(f)\n",
    "        relation_kg = pkl_file['temporal_kg']\n",
    "\n",
    "    for i in range(len(list_df)):\n",
    "        cur_data = []\n",
    "        start_time, end_time = list_df[i][6].iloc[-1], list_df[i][6].iloc[-1] \n",
    "        start_time = pd.to_datetime(start_time, utc=True)\n",
    "        end_time = pd.to_datetime(end_time, utc=True) + pd.offsets.Day(1)\n",
    "        start_time.tz_localize(None)\n",
    "        end_time.tz_localize(None)\n",
    "        relation_kg['expiry_ts'] = pd.to_datetime(relation_kg['expiry_ts'], utc=True)\n",
    "        relation_kg['timestamp'] = pd.to_datetime(relation_kg['timestamp'], utc=True)\n",
    "        non_temporal_time = pd.to_datetime('1970-01-01', utc=True)\n",
    "        mask = (relation_kg['timestamp'] >= start_time) & (relation_kg['timestamp'] < end_time) & (relation_kg['expiry_ts'] >= end_time) | (relation_kg['timestamp'] == non_temporal_time)\n",
    "        tkg = relation_kg.loc[mask]\n",
    "\n",
    "        head, relation, tail = torch.Tensor([int(x) for x in tkg['head'].values]).long(), torch.Tensor([int(x) for x in tkg['relation'].values]).long(), torch.Tensor([int(x) for x in tkg['tail'].values]).long()\n",
    "        #head, relation, tail = head.to(device), relation.to(device), tail.to(device)\n",
    "        #print(start_time, tkg)\n",
    "\n",
    "        start_ts_years = torch.Tensor(tkg['timestamp'].dt.year.values).long() - 2000\n",
    "        start_ts_years[start_ts_years <= 0] = 0\n",
    "        start_ts_months = torch.Tensor(tkg['timestamp'].dt.month.values).long()\n",
    "        start_ts_months[start_ts_years <= 0] = 0\n",
    "        start_ts_days = torch.Tensor(tkg['timestamp'].dt.day.values).long()\n",
    "        start_ts_days[start_ts_years <= 0] = 0\n",
    "        start_ts_hours = torch.Tensor(tkg['timestamp'].dt.hour.values).long()\n",
    "        start_ts_hours[start_ts_years <= 0] = 0\n",
    "        start_ts_minutes = torch.Tensor(tkg['timestamp'].dt.minute.values).long()\n",
    "        start_ts_minutes[start_ts_years <= 0] = 0\n",
    "        start_ts_seconds = torch.Tensor(tkg['timestamp'].dt.second.values).long()\n",
    "        start_ts_seconds[start_ts_years <= 0] = 0\n",
    "\n",
    "        ts = torch.stack([start_ts_years, start_ts_months, start_ts_days, start_ts_hours, start_ts_minutes, start_ts_seconds], dim=1)\n",
    "        \n",
    "        #print(tkg['timestamp'], ts, ts.shape)\n",
    "        temporal_kg = (head, relation, tail, ts)\n",
    "\n",
    "        for j in range(company_id):\n",
    "            cur_data.append(df_map[j][i])\n",
    "\n",
    "        dataset.append((cur_data, temporal_kg)) \n",
    "\n",
    "    print(\"Skipped Tickers: \", skipped_ticker)\n",
    "    \n",
    "    \n",
    "    #Created from where?\n",
    "    sector_graph = open(\"/data6/kpnaveen/Phase-Stock-KG/kg/sector/sector_hypergraph_\"+INDEX+\".txt\", \"r\").readlines()\n",
    "\n",
    "    # Unidirectional Homogeneous Graph\n",
    "    sector_map = {}\n",
    "    graph_dataset = []\n",
    "    for lines in sector_graph[1:]:\n",
    "        lines = lines[:-1]\n",
    "        tickers = lines.split(\"\\t\")[2:]\n",
    "        for i in range(len(tickers)):\n",
    "            for j in range(i+1, len(tickers)):\n",
    "                if tickers[i] not in company_to_id or tickers[j] not in company_to_id:\n",
    "                    continue\n",
    "                if tickers[i] + \"-\" + tickers[j] not in sector_map:\n",
    "                    graph_dataset.append([company_to_id[tickers[i]], company_to_id[tickers[j]]])\n",
    "                    graph_dataset.append([company_to_id[tickers[j]], company_to_id[tickers[i]]])\n",
    "                    sector_map[tickers[i] + \"-\" + tickers[j]] = 1\n",
    "                    sector_map[tickers[j] + \"-\" + tickers[i]] = 1\n",
    "\n",
    "    edge_index = torch.Tensor(graph_dataset).t().contiguous()\n",
    "    x = torch.randn(len(company_to_id.items()), 8)\n",
    "\n",
    "    print(\"Number of edges in pairwise graph: \", len(graph_dataset))\n",
    "\n",
    "    graph_data = Data(x=x, edge_index=edge_index.long())\n",
    "\n",
    "    hyperedge_index, hyper_x = get_sector_hypergraph(company_to_id)\n",
    "    hyper_data = {\n",
    "        'hyperedge_index': hyperedge_index,\n",
    "        'x': hyper_x\n",
    "    }\n",
    "    \n",
    "    return dataset, company_to_id, graph_data, hyper_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_graph(save_path):\n",
    "    with open(save_path, 'rb') as handle:\n",
    "        b = pd.read_pickle(handle)\n",
    "\n",
    "    dataset = b['train']\n",
    "    company_to_id = b['company']\n",
    "    graph = b['graph']\n",
    "    hyper_data = b['hyper_graph']\n",
    "\n",
    "    return dataset, company_to_id, graph, hyper_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_graph(save_path, dataset, company_to_id, graph, hyper_data):\n",
    "    print(\"--- Saving Dataset ---\")\n",
    "    save_data = {'train': dataset, 'company': company_to_id, 'graph': graph,\n",
    "                    'hyper_graph': hyper_data}\n",
    "\n",
    "    with open(save_path, 'wb') as handle:\n",
    "        pickle.dump(save_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_dataset_graph(INDEX, W, T, save_path, problem, fast):\n",
    "    if fast == True:\n",
    "        print(\"--- Creating Dataset ---\")\n",
    "        dataset, company_to_id, graph, hyper_data = create_batch_dataset(INDEX, W, T, problem, fast)\n",
    "    elif os.path.isfile(save_path):\n",
    "        print(\"--- File exists: Loading Dataset ---\")\n",
    "        dataset, company_to_id, graph, hyper_data = load_dataset_graph(save_path)\n",
    "    else:\n",
    "        print(\"--- Creating Dataset ---\")\n",
    "        dataset, company_to_id, graph, hyper_data = create_batch_dataset(INDEX, W, T, problem, fast)\n",
    "        save_dataset_graph(save_path, dataset, company_to_id, graph, hyper_data)\n",
    "\n",
    "    return dataset, company_to_id, graph, hyper_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS UTILS\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return (((y_true - y_pred) / y_true).abs()).mean() * 100\n",
    "    #return ((y_true - y_pred).abs()).mean() * 100\n",
    "\n",
    "def root_mean_square_error(y_true, y_pred, scale = None): \n",
    "    if scale == None:\n",
    "        return ((y_true - y_pred) ** 2).mean() ** (1/2)\n",
    "    else:\n",
    "        return (((y_true - y_pred)*scale) ** 2).mean() ** (1/2)\n",
    "\n",
    "def mean_square_error(y_true, y_pred, scale = None):\n",
    "    if scale == None:\n",
    "        return ((y_true - y_pred) ** 2).mean() \n",
    "    else:\n",
    "        return (((y_true - y_pred)*scale.unsqueeze(dim=1)) ** 2).mean() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. KG Loader\n",
    "\n",
    "def get_sector_hypergraph(company_to_id):\n",
    "    # HyperGraph\n",
    "    sector_graph = open(\"/data6/kpnaveen/Phase-Stock-KG/kg/sector/sector_hypergraph_nasdaq100.txt\", \"r\").readlines()\n",
    "\n",
    "    n = len(company_to_id.items())\n",
    "    hyperedge_index = [[], []]\n",
    "    edge = 0\n",
    "    for lines in sector_graph[1:]:\n",
    "        lines = lines[:-1]\n",
    "        tickers = lines.split(\"\\t\")[2:]\n",
    "        for i in range(len(tickers)):\n",
    "            if tickers[i] not in company_to_id:\n",
    "                continue\n",
    "            hyperedge_index[0].extend([company_to_id[tickers[i]]])\n",
    "            hyperedge_index[1].extend([edge])\n",
    "        edge += 1\n",
    "\n",
    "    hyperedge_index = torch.Tensor(hyperedge_index).long()\n",
    "    hyper_x = torch.randn(len(company_to_id.items()), 8)\n",
    "\n",
    "    return hyperedge_index, hyper_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL-Apple Inc.csv\n",
      "ADI-Analog Devices Inc.csv\n",
      "ADP-Automatic Data Processing Inc.csv\n",
      "AMAT-Applied Material Inc.csv\n",
      "AVGO-Broadcom Inc..csv\n",
      "ATVI-Activision Blizzard Inc.csv\n",
      "ASML-ASML Holding NV ADRs.csv\n",
      "AEP-American Electric Power Inc.csv\n",
      "AMGN-Amgen Inc.csv\n",
      "ABNB-Airbnb, Inc..csv\n",
      "Skipping file: Less Training-Testing samples [518 samples]\n",
      "ADBE-Adobe Inc.csv\n",
      "ALGN-Align Technology Inc.csv\n",
      "AMD-Advanced Micro Devices Inc.csv\n",
      "BIDU-Baidu Inc ADRs of Class A.csv\n",
      "ADSK-Autodesk Inc.csv\n",
      "AMZN-Amazon.Com Inc..csv\n",
      "ANSS-Ansys Inc.csv\n",
      "BKNG-Booking Holdings Inc.csv\n",
      "AZN-AstraZeneca PLC ADRs.csv\n",
      "BIIB-Biogen Inc.csv\n",
      "Skipped Tickers:  ['ABNB']\n",
      "Number of edges in pairwise graph:  80\n",
      "2 19 (array([1.02220015, 1.00836081, 0.99507301, 0.97269904, 0.97432721,\n",
      "       0.98349213, 0.96749949, 0.97781985, 0.98698478, 1.00129676,\n",
      "       1.00457934, 1.02372324, 1.03333456, 1.03076103, 1.02398581,\n",
      "       1.02797743, 1.0316539 , 1.0285814 , 1.03204779, 1.01550368,\n",
      "       1.0066539 , 0.99885456, 1.00439551, 1.01828728, 1.04173794,\n",
      "       1.04255199, 1.0496161 , 1.05857096, 1.06852368, 1.05935875,\n",
      "       1.05964757, 1.07511507, 1.07669066, 1.08968963, 1.10234721,\n",
      "       1.11739449, 1.1184711 , 1.10998897, 1.1089386 , 1.10216338,\n",
      "       1.11398059, 1.12122846, 1.12960559, 1.12262029, 1.10996272,\n",
      "       1.11634404, 1.19339235, 1.17742596, 1.16686926, 1.17046691]), array([1.00422057, 0.98737844, 0.97559407, 0.95425908, 0.97030527,\n",
      "       0.96493941, 0.95289833, 0.96719874, 0.97261594, 0.98147342,\n",
      "       0.99872631, 1.01061334, 1.01775071, 1.01318078, 1.00370708,\n",
      "       1.01541439, 1.01166598, 1.01130654, 1.01515768, 0.99456714,\n",
      "       0.98522183, 0.98627447, 0.98807165, 1.01695485, 1.02003572,\n",
      "       1.02473404, 1.03618463, 1.05030526, 1.04814865, 1.04147344,\n",
      "       1.04309089, 1.0590601 , 1.06465701, 1.0745928 , 1.08537586,\n",
      "       1.09821287, 1.09371993, 1.08563264, 1.08576095, 1.07947084,\n",
      "       1.09369426, 1.10262874, 1.10750678, 1.09757099, 1.10001004,\n",
      "       1.09140926, 1.16676219, 1.1522308 , 1.15143494, 1.16535018]), array([1.01284432, 0.98960015, 0.9879662 , 0.96432673, 0.97758277,\n",
      "       0.96688307, 0.95750106, 0.97597516, 0.97563262, 0.99697926,\n",
      "       1.00343597, 1.02407111, 1.02884118, 1.02617947, 1.01927469,\n",
      "       1.02841954, 1.03052782, 1.02636394, 1.02017074, 0.9953453 ,\n",
      "       0.9970056 , 1.00032619, 1.0027244 , 1.02059238, 1.03311051,\n",
      "       1.04388931, 1.05282326, 1.06212617, 1.05769871, 1.05551139,\n",
      "       1.06336482, 1.07788586, 1.07862377, 1.08755772, 1.10481958,\n",
      "       1.11043294, 1.11082823, 1.10505674, 1.10358092, 1.10334376,\n",
      "       1.11467592, 1.12347815, 1.12403159, 1.1062163 , 1.11293659,\n",
      "       1.10568928, 1.16941306, 1.16785821, 1.16951851, 1.17378783]), array([0.98956842, 0.97063985, 0.96428741, 0.94903658, 0.9683252 ,\n",
      "       0.9438415 , 0.93504587, 0.96732216, 0.95981243, 0.98295883,\n",
      "       0.99769535, 1.00224745, 1.01076015, 1.00546223, 1.00067862,\n",
      "       1.00471642, 1.01232898, 1.00775117, 0.99995851, 0.97778945,\n",
      "       0.97457467, 0.97992407, 0.98298453, 1.01832129, 1.01960726,\n",
      "       1.02500808, 1.03730135, 1.04553119, 1.03552679, 1.04190486,\n",
      "       1.0415963 , 1.05761872, 1.0633024 , 1.07510707, 1.08634589,\n",
      "       1.08462281, 1.08850622, 1.08673165, 1.08374838, 1.07968488,\n",
      "       1.09226108, 1.1036028 , 1.10010516, 1.08094507, 1.09923073,\n",
      "       1.081228  , 1.14873822, 1.14351743, 1.15033276, 1.16506922]), array([0.81058878, 1.113947  , 0.86334419, 1.03985415, 0.94942188,\n",
      "       0.99410507, 0.59135193, 0.8040959 , 0.87243681, 0.94226607,\n",
      "       0.89872677, 0.87986559, 0.82915749, 0.65828898, 0.70737874,\n",
      "       0.8736002 , 0.68938197, 0.69883856, 0.78672308, 0.94446936,\n",
      "       0.59469261, 0.97833759, 0.54671424, 0.78274547, 0.6103561 ,\n",
      "       0.46971617, 0.62533066, 0.61529562, 0.53077128, 0.50132912,\n",
      "       0.41703226, 0.7015163 , 0.60356426, 0.62967224, 0.7388227 ,\n",
      "       0.91461124, 0.59932666, 0.49925582, 0.49345838, 0.52464237,\n",
      "       0.56381409, 0.642489  , 0.60754837, 0.96091927, 0.71043344,\n",
      "       1.27117983, 2.22444284, 0.75203593, 0.69568637, 0.88052853]), 0, 2237    2011-11-16\n",
      "2238    2011-11-17\n",
      "2239    2011-11-18\n",
      "2240    2011-11-21\n",
      "2241    2011-11-22\n",
      "2242    2011-11-23\n",
      "2243    2011-11-25\n",
      "2244    2011-11-28\n",
      "2245    2011-11-29\n",
      "2246    2011-11-30\n",
      "2247    2011-12-01\n",
      "2248    2011-12-02\n",
      "2249    2011-12-05\n",
      "2250    2011-12-06\n",
      "2251    2011-12-07\n",
      "2252    2011-12-08\n",
      "2253    2011-12-09\n",
      "2254    2011-12-12\n",
      "2255    2011-12-13\n",
      "2256    2011-12-14\n",
      "2257    2011-12-15\n",
      "2258    2011-12-16\n",
      "2259    2011-12-19\n",
      "2260    2011-12-20\n",
      "2261    2011-12-21\n",
      "2262    2011-12-22\n",
      "2263    2011-12-23\n",
      "2264    2011-12-27\n",
      "2265    2011-12-28\n",
      "2266    2011-12-29\n",
      "2267    2011-12-30\n",
      "2268    2012-01-03\n",
      "2269    2012-01-04\n",
      "2270    2012-01-05\n",
      "2271    2012-01-06\n",
      "2272    2012-01-09\n",
      "2273    2012-01-10\n",
      "2274    2012-01-11\n",
      "2275    2012-01-12\n",
      "2276    2012-01-13\n",
      "2277    2012-01-17\n",
      "2278    2012-01-18\n",
      "2279    2012-01-19\n",
      "2280    2012-01-20\n",
      "2281    2012-01-23\n",
      "2282    2012-01-24\n",
      "2283    2012-01-25\n",
      "2284    2012-01-26\n",
      "2285    2012-01-27\n",
      "2286    2012-01-30\n",
      "Name: Date, dtype: object, [array([1.00765984]), array([1.02419369]), array([1.1818945])], 2286    16.178928\n",
      "Name: Close, dtype: float64, array([], shape=(1, 0), dtype=float64), [array([15.906786]), array([16.18107]), array([16.364286]), array([18.780357])], [array([16.210714]), array([16.365713]), array([16.606428]), array([19.121786])])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument('--index', type=str, default=\"AAPL\")\n",
    "    #parser.add_argument('--window', type=int, default=10)\n",
    "    #parser.add_argument('--test_size', type=float, default=0.2)\n",
    "\n",
    "    #create_dataset(\"nasdaq100\", 50, 5)\n",
    "    # d, s, c, g, h = create_batch_dataset(\"nasdaq100\", 50, 5)   \n",
    "    d, s, c, g = create_batch_dataset(\"nasdaq100\", 50, 5)     #---> naveen changed\n",
    "    print(len(d[0]), len(d[0][0]), d[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocknaveen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
