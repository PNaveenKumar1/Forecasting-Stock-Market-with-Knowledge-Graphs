{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/kawinm/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment SP500 W=20 Run 1 Without Relation KG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkawinm\u001b[0m (\u001b[33mstatsml-csa-iisc\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "wandb: ERROR Failed to sample metric: Not Supported\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/raid/home/kawinm/Phase-Stock-KG/wandb/run-20230324_060217-24on7imh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/statsml-csa-iisc/KG-Stock-Graph250/runs/24on7imh\" target=\"_blank\">smooth-snowflake-12</a></strong> to <a href=\"https://wandb.ai/statsml-csa-iisc/KG-Stock-Graph250\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File exists: Loading Dataset ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to serialize metric: division by zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph details:  torch.Size([83, 8]) torch.Size([2, 304])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os, sys\n",
    "import pickle\n",
    "import math\n",
    "from queue import PriorityQueue\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from utils import (mean_absolute_percentage_error,\n",
    "                   load_or_create_dataset_graph,\n",
    "                   mean_square_error, root_mean_square_error)\n",
    "\n",
    "from models.models import Transformer_Ranking\n",
    "\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "from random import randint\n",
    "import wandb\n",
    "\n",
    "GPU = 1\n",
    "LR = 0.0001\n",
    "BS = 128\n",
    "W = 20\n",
    "T = 250\n",
    "LOG = False\n",
    "D_MODEL = 5\n",
    "N_HEAD  = 5\n",
    "DROPOUT = 0.2\n",
    "D_FF    = 64\n",
    "ENC_LAYERS = 1\n",
    "DEC_LAYERS = 1\n",
    "MAX_EPOCH = 50\n",
    "USE_POS_ENCODING = False\n",
    "USE_GRAPH = True\n",
    "HYPER_GRAPH = True\n",
    "USE_KG = False\n",
    "PREDICTION_PROBLEM = 'value'\n",
    "RUN = randint(1, 100000)\n",
    "\n",
    "tau_choices = [50, 75, 125, 250]\n",
    "tau_positions = [1, 5, 20, 50, 75, 100, 125, 250]\n",
    "\n",
    "MODEL = \"ours\"\n",
    "\n",
    "print(\"Experiment SP500 W=20 Run 1 Without Relation KG\")\n",
    "FAST = False\n",
    "if FAST == True:\n",
    "    LOG = False\n",
    "\n",
    "LOG = True\n",
    "if LOG:\n",
    "    wandb_config = {\n",
    "        'Prediction Problem': PREDICTION_PROBLEM,\n",
    "        'Learning Rate':LR,\n",
    "        'Batch Size': BS,\n",
    "        'Window Size': W,\n",
    "        'Prediction Steps': T,\n",
    "        'Model': MODEL,\n",
    "        'Optimizer': 'ADAM',\n",
    "        'Preprocessing': ['Window Divide Min'],\n",
    "        'KG': ['NO Company'],\n",
    "        'Transformer Params': [D_MODEL, N_HEAD, DROPOUT, D_FF, ENC_LAYERS, DEC_LAYERS, USE_POS_ENCODING]\n",
    "    }\n",
    "\n",
    "    wandb.init(project=\"KG-Stock-Graph\"+str(T), config=wandb_config)\n",
    "\n",
    "INDEX = \"nasdaq100\" \n",
    "\n",
    "save_path = \"data/pickle/\"+INDEX+\"/graph_data-P25-W\"+str(W)+\"-T\"+str(T)+\"_\"+str(PREDICTION_PROBLEM)+\".pkl\"\n",
    "\n",
    "\n",
    "dataset, company_to_id, graph, hyper_data = load_or_create_dataset_graph(INDEX=INDEX, W=W, T=T, save_path=save_path, problem=PREDICTION_PROBLEM, fast=FAST)\n",
    "\n",
    "\n",
    "num_nodes = len(company_to_id.keys())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:\"+str(GPU))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "if not HYPER_GRAPH:\n",
    "    graph_nodes_batch = torch.zeros(graph.x.shape[0]).to(device)\n",
    "    graph = graph.to(device)\n",
    "    graph_data = {\n",
    "        'x': graph.x,\n",
    "        'edge_list': graph.edge_index,\n",
    "        'batch': graph_nodes_batch\n",
    "    }\n",
    "else:\n",
    "    x, hyperedge_index = hyper_data['x'].to(device), hyper_data['hyperedge_index'].to(device)\n",
    "\n",
    "    print(\"Graph details: \", x.shape, hyperedge_index.shape)\n",
    "    graph_data = {\n",
    "        'x': x,\n",
    "        'hyperedge_index': hyperedge_index\n",
    "    }\n",
    "\n",
    "if USE_KG:\n",
    "    with open('./kg/profile_and_relationship/wikidata/'+INDEX+'_relations_kg.pkl', 'rb') as f:\n",
    "        relation_kg = pickle.load(f)['kg']\n",
    "    head, relation, tail = relation_kg[0], relation_kg[1], relation_kg[2]\n",
    "    head, relation, tail = head.to(device), relation.to(device), tail.to(device)\n",
    "    relation_kg = (head, relation, tail)\n",
    "else:\n",
    "    relation_kg = None\n",
    "\n",
    "def rank_loss(prediction, ground_truth):\n",
    "    all_one = torch.ones(prediction.shape[0], 1, dtype=torch.float32).to(device)\n",
    "    prediction = prediction.unsqueeze(dim=1)\n",
    "    ground_truth = ground_truth.unsqueeze(dim=1)\n",
    "    #print(prediction.shape, ground_truth.shape, base_price.shape)\n",
    "    return_ratio = prediction - 1\n",
    "    true_return_ratio = ground_truth - 1\n",
    "\n",
    "    pre_pw_dif = torch.sub(\n",
    "        return_ratio @ all_one.t(),                  # C x C\n",
    "        all_one @ return_ratio.t()                   # C x C\n",
    "    )\n",
    "    gt_pw_dif = torch.sub(\n",
    "        all_one @ true_return_ratio.t(),\n",
    "        true_return_ratio @ all_one.t()\n",
    "    )\n",
    "\n",
    "    rank_loss = torch.mean(\n",
    "        F.relu(-1*pre_pw_dif * gt_pw_dif )\n",
    "    )\n",
    "   \n",
    "    return rank_loss \n",
    "\n",
    "def evaluate(prediction, ground_truth, K):\n",
    "    return_ratio = prediction - 1\n",
    "    true_return_ratio = ground_truth - 1\n",
    "\n",
    "    #print(\"True top k: \", torch.topk(true_return_ratio.squeeze(), k=3, dim=0))\n",
    "    #print(\"Predicted top k: \", torch.topk(return_ratio.squeeze(), k=3, dim=0))\n",
    "    \n",
    "    obtained_return_ratio = true_return_ratio[torch.topk(return_ratio, k=K, dim=0)[1]].mean()\n",
    "\n",
    "    #return_ratio = -1*return_ratio\n",
    "    #obtained_return_ratio += true_return_ratio[torch.topk(return_ratio.squeeze(), k=K, dim=0)[1]].mean()\n",
    "    #obtained_return_ratio /= 2\n",
    "\n",
    "    target_obtained_return_ratio = torch.topk(true_return_ratio, k=K, dim=0)[0].mean()\n",
    "\n",
    "    expected_return_ratio = torch.topk(return_ratio.squeeze(), k=K, dim=0)[0].mean()\n",
    "\n",
    "    random = torch.randint(0, prediction.shape[0]-1, (K,))\n",
    "    random_return_ratio = true_return_ratio[random].mean()\n",
    "\n",
    "    a_cat_b, counts = torch.cat([torch.topk(return_ratio.squeeze(), k=K, dim=0)[1], torch.topk(true_return_ratio.squeeze(), k=K, dim=0)[1]]).unique(return_counts=True)\n",
    "    accuracy = a_cat_b[torch.where(counts.gt(1))].shape[0] / K\n",
    "\n",
    "    return obtained_return_ratio, target_obtained_return_ratio, expected_return_ratio, random_return_ratio, accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau:  50 Tau Position:  3\n",
      "2530\n",
      "Phase:  1\n",
      "Epoch: 1\n",
      "[TRAINING] Movement Prediction Accuracy: 0.3941686451435089, MAPE: 2174.150390625\n",
      "[TRAINING] Range of predictions min: -0.10264767706394196 max: 0.35429006814956665\n",
      "[TRAINING] Epoch: 1 MSE: 0.854455623626709 RMSE: 0.9243676885453694 Loss: 0.854455623626709 MAE: 0.9108234643936157\n",
      "[TRAINING] Top 1 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 3 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 5 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 10 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[VALIDATION] Movement Prediction Accuracy: 0.20433732867240906, MAPE: 511.7800598144531\n",
      "[VALIDATION] Range of predictions min: 0.10720691829919815 max: 0.20837636291980743\n",
      "[VALIDATION] Epoch: 1 MSE: 0.8467381489276886 RMSE: 0.9201837582394554 Loss: 0.8467381489276886 MAE: 0.909945662021637\n",
      "[VALIDATION] Top 1 Return Ratio: 0.17953437566757202 True Return Ratio: 0.7546033263206482 Expected Return Ratio: -0.7950437068939209 Random Return Ratio: 0.06863529980182648 Accuracy: 0.0\n",
      "[VALIDATION] Top 3 Return Ratio: 0.1225106418132782 True Return Ratio: 0.5009898543357849 Expected Return Ratio: -0.7997759580612183 Random Return Ratio: 0.07413619011640549 Accuracy: 0.0\n",
      "[VALIDATION] Top 5 Return Ratio: 0.10830006003379822 True Return Ratio: 0.42307981848716736 Expected Return Ratio: -0.8018350601196289 Random Return Ratio: 0.07564663887023926 Accuracy: 0.0279999990016222\n",
      "[VALIDATION] Top 10 Return Ratio: 0.0869741216301918 True Return Ratio: 0.3334497809410095 Expected Return Ratio: -0.8044223785400391 Random Return Ratio: 0.08349001407623291 Accuracy: 0.04800000041723251\n",
      "Epoch: 2\n",
      "[TRAINING] Movement Prediction Accuracy: 0.3941686451435089, MAPE: 403.39154052734375\n",
      "[TRAINING] Range of predictions min: -0.013857156038284302 max: 0.4339076280593872\n",
      "[TRAINING] Epoch: 2 MSE: 0.670294439792633 RMSE: 0.8187151151607213 Loss: 0.670294439792633 MAE: 0.8033094501495361\n",
      "[TRAINING] Top 1 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 3 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 5 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 10 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[VALIDATION] Movement Prediction Accuracy: 0.20433732867240906, MAPE: 283.00457763671875\n",
      "[VALIDATION] Range of predictions min: 0.2141655683517456 max: 0.3158568739891052\n",
      "[VALIDATION] Epoch: 2 MSE: 0.6650270557403565 RMSE: 0.8154919102850479 Loss: 0.6650270557403565 MAE: 0.8039201271533966\n",
      "[VALIDATION] Top 1 Return Ratio: 0.17953433096408844 True Return Ratio: 0.754603385925293 Expected Return Ratio: -0.6873725652694702 Random Return Ratio: 0.10353682935237885 Accuracy: 0.0\n",
      "[VALIDATION] Top 3 Return Ratio: 0.11577899754047394 True Return Ratio: 0.5009897947311401 Expected Return Ratio: -0.691798210144043 Random Return Ratio: 0.09411754459142685 Accuracy: 0.0\n",
      "[VALIDATION] Top 5 Return Ratio: 0.09715630114078522 True Return Ratio: 0.42307981848716736 Expected Return Ratio: -0.6937463879585266 Random Return Ratio: 0.1086818277835846 Accuracy: 0.01600000075995922\n",
      "[VALIDATION] Top 10 Return Ratio: 0.08152557909488678 True Return Ratio: 0.3334498107433319 Expected Return Ratio: -0.696843683719635 Random Return Ratio: 0.08734734356403351 Accuracy: 0.04399999976158142\n",
      "Epoch: 3\n",
      "[TRAINING] Movement Prediction Accuracy: 0.3941686451435089, MAPE: 220.13121032714844\n",
      "[TRAINING] Range of predictions min: 0.10505226254463196 max: 0.537612795829773\n",
      "[TRAINING] Epoch: 3 MSE: 0.5153343796730041 RMSE: 0.717867940273839 Loss: 0.5153343796730041 MAE: 0.6999433732032776\n",
      "[TRAINING] Top 1 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 3 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 5 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 10 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[VALIDATION] Movement Prediction Accuracy: 0.20433729887008667, MAPE: 181.53466796875\n",
      "[VALIDATION] Range of predictions min: 0.3188619613647461 max: 0.421014666557312\n",
      "[VALIDATION] Epoch: 3 MSE: 0.5109174102544785 RMSE: 0.7147848699115549 Loss: 0.5109174102544785 MAE: 0.7015530645847321\n",
      "[VALIDATION] Top 1 Return Ratio: 0.17764517664909363 True Return Ratio: 0.7546032667160034 Expected Return Ratio: -0.5824875235557556 Random Return Ratio: 0.10244690626859665 Accuracy: 0.0\n",
      "[VALIDATION] Top 3 Return Ratio: 0.10848958045244217 True Return Ratio: 0.5009897947311401 Expected Return Ratio: -0.5860273241996765 Random Return Ratio: 0.08112283796072006 Accuracy: 0.0\n",
      "[VALIDATION] Top 5 Return Ratio: 0.09837913513183594 True Return Ratio: 0.42307984828948975 Expected Return Ratio: -0.5882805585861206 Random Return Ratio: 0.08079050481319427 Accuracy: 0.01600000075995922\n",
      "[VALIDATION] Top 10 Return Ratio: 0.07958804070949554 True Return Ratio: 0.3334498107433319 Expected Return Ratio: -0.5922902226448059 Random Return Ratio: 0.10027236491441727 Accuracy: 0.04200000315904617\n",
      "Epoch: 4\n",
      "[TRAINING] Movement Prediction Accuracy: 0.3941686451435089, MAPE: 141.51904296875\n",
      "[TRAINING] Range of predictions min: 0.19169598817825317 max: 0.6319262981414795\n",
      "[TRAINING] Epoch: 4 MSE: 0.38235299348831175 RMSE: 0.61834698470059 Loss: 0.38235299348831175 MAE: 0.5977426052093506\n",
      "[TRAINING] Top 1 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 3 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 5 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 10 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[VALIDATION] Movement Prediction Accuracy: 0.20433729887008667, MAPE: 122.5051040649414\n",
      "[VALIDATION] Range of predictions min: 0.42404812574386597 max: 0.5256698727607727\n",
      "[VALIDATION] Epoch: 4 MSE: 0.3775371038913727 RMSE: 0.6144404803488884 Loss: 0.3775371038913727 MAE: 0.5989759993553162\n",
      "[VALIDATION] Top 1 Return Ratio: 0.12209784984588623 True Return Ratio: 0.7546035051345825 Expected Return Ratio: -0.47733137011528015 Random Return Ratio: 0.06709989160299301 Accuracy: 0.0\n",
      "[VALIDATION] Top 3 Return Ratio: 0.09940856695175171 True Return Ratio: 0.5009898543357849 Expected Return Ratio: -0.4798632562160492 Random Return Ratio: 0.07962335646152496 Accuracy: 0.0\n",
      "[VALIDATION] Top 5 Return Ratio: 0.0874868780374527 True Return Ratio: 0.42307984828948975 Expected Return Ratio: -0.48236238956451416 Random Return Ratio: 0.08175424486398697 Accuracy: 0.01600000075995922\n",
      "[VALIDATION] Top 10 Return Ratio: 0.07783006131649017 True Return Ratio: 0.3334498405456543 Expected Return Ratio: -0.48725345730781555 Random Return Ratio: 0.08531873673200607 Accuracy: 0.03800000622868538\n",
      "Epoch: 5\n",
      "[TRAINING] Movement Prediction Accuracy: 0.3941687047481537, MAPE: 94.60694885253906\n",
      "[TRAINING] Range of predictions min: 0.2506367564201355 max: 0.7050392031669617\n",
      "[TRAINING] Epoch: 5 MSE: 0.27125457882881165 RMSE: 0.5208210621977684 Loss: 0.27125457882881165 MAE: 0.4964024555683136\n",
      "[TRAINING] Top 1 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 3 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 5 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 10 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[VALIDATION] Movement Prediction Accuracy: 0.20433729887008667, MAPE: 84.15664672851562\n",
      "[VALIDATION] Range of predictions min: 0.5292114615440369 max: 0.6337656378746033\n",
      "[VALIDATION] Epoch: 5 MSE: 0.2658806109428406 RMSE: 0.5156361226124878 Loss: 0.2658806109428406 MAE: 0.4970594477653503\n",
      "[VALIDATION] Top 1 Return Ratio: 0.07645700126886368 True Return Ratio: 0.7546032667160034 Expected Return Ratio: -0.3711707592010498 Random Return Ratio: 0.06887674331665039 Accuracy: 0.0\n",
      "[VALIDATION] Top 3 Return Ratio: 0.0904034897685051 True Return Ratio: 0.5009898543357849 Expected Return Ratio: -0.37370702624320984 Random Return Ratio: 0.07625608891248703 Accuracy: 0.0\n",
      "[VALIDATION] Top 5 Return Ratio: 0.07870703190565109 True Return Ratio: 0.4230799078941345 Expected Return Ratio: -0.3763962984085083 Random Return Ratio: 0.09602236747741699 Accuracy: 0.01600000075995922\n",
      "[VALIDATION] Top 10 Return Ratio: 0.07838068157434464 True Return Ratio: 0.3334498405456543 Expected Return Ratio: -0.3824290335178375 Random Return Ratio: 0.08854205161333084 Accuracy: 0.04000000283122063\n",
      "Epoch: 6\n",
      "[TRAINING] Movement Prediction Accuracy: 0.3941686153411865, MAPE: 63.48786544799805\n",
      "[TRAINING] Range of predictions min: 0.3841105103492737 max: 0.8063161969184875\n",
      "[TRAINING] Epoch: 6 MSE: 0.18145849525928498 RMSE: 0.42597945403421156 Loss: 0.18145849525928498 MAE: 0.3966396474838257\n",
      "[TRAINING] Top 1 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 3 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 5 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[TRAINING] Top 10 Return Ratio: 0.0 True Return Ratio: 0.0 Expected Return Ratio: 0.0 Random Return Ratio: 0.0 Accuracy: 0.0\n",
      "[VALIDATION] Movement Prediction Accuracy: 0.20433734357357025, MAPE: 57.43043899536133\n",
      "[VALIDATION] Range of predictions min: 0.6325420141220093 max: 0.7422322630882263\n",
      "[VALIDATION] Epoch: 6 MSE: 0.17620960325002671 RMSE: 0.4197732760074499 Loss: 0.17620960325002671 MAE: 0.39663431167602536\n",
      "[VALIDATION] Top 1 Return Ratio: 0.0505782775580883 True Return Ratio: 0.7546033263206482 Expected Return Ratio: -0.26464879512786865 Random Return Ratio: 0.09800487011671066 Accuracy: 0.0\n",
      "[VALIDATION] Top 3 Return Ratio: 0.08048970997333527 True Return Ratio: 0.5009898543357849 Expected Return Ratio: -0.26809751987457275 Random Return Ratio: 0.09373147040605545 Accuracy: 0.0\n",
      "[VALIDATION] Top 5 Return Ratio: 0.07474835962057114 True Return Ratio: 0.42307984828948975 Expected Return Ratio: -0.2710854113101959 Random Return Ratio: 0.08735515177249908 Accuracy: 0.01600000075995922\n",
      "[VALIDATION] Top 10 Return Ratio: 0.07720161229372025 True Return Ratio: 0.3334498405456543 Expected Return Ratio: -0.2784920930862427 Random Return Ratio: 0.09057239443063736 Accuracy: 0.03800000622868538\n",
      "Epoch: 7\n"
     ]
    }
   ],
   "source": [
    "from models.models import Saturation\n",
    "\n",
    "top_k_choice = [1, 3, 5, 10]\n",
    "\n",
    "# ----------- Main Training Loop -----------\n",
    "def predict(loader, desc):\n",
    "    epoch_loss, mape, move_loss, rmse_returns_loss, mae_loss = 0, 0, 0, 0, 0\n",
    "    mini, maxi = float(\"infinity\"), 0\n",
    "    rr, true_rr, exp_rr, ran_rr, accuracy = torch.zeros(4).to(device), torch.zeros(4).to(device), torch.zeros(4).to(device), torch.zeros(4).to(device), torch.zeros(4).to(device)\n",
    "    \n",
    "    #tqdm_loader = tqdm(loader)\n",
    "\n",
    "    wandb.watch(model, log=\"all\") \n",
    "    \n",
    "    yb_store, yhat_store = [], []\n",
    "    for xb, company, yb, scale, move_target in loader:\n",
    "        xb      = xb.to(device) \n",
    "        yb      = yb.to(device) \n",
    "        scale   = scale.to(device)\n",
    "        move_target = move_target.to(device)\n",
    "\n",
    "        y_hat, kg_loss, hold_pred = model(xb, yb, graph_data, relation_kg)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        true_return_ratio = yb.squeeze() \n",
    "\n",
    "        #print(\"TRR: \", float(true_return_ratio.min()), float(true_return_ratio.max()))\n",
    "        #print(\"Pred: \", float(y_hat.min()), float(y_hat.max()))\n",
    "\n",
    "        loss = F.mse_loss(y_hat, true_return_ratio) #+ rank_loss(y_hat, true_return_ratio)\n",
    "        if USE_KG:\n",
    "            loss += kg_loss.mean()\n",
    "\n",
    "        if model.training:\n",
    "            opt_c.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt_c.step()\n",
    "\n",
    "\n",
    "        yb_store.append(yb.detach().cpu().numpy())\n",
    "        yhat_store.append(y_hat.detach().cpu().numpy())\n",
    "        \n",
    "        if not model.training:\n",
    "            for index, k in enumerate(top_k_choice):\n",
    "                crr, ctrr, cerr, crrr, cacc = evaluate(y_hat, true_return_ratio, k)\n",
    "                ran_rr[index] += crrr\n",
    "                true_rr[index] += ctrr\n",
    "                rr[index] += crr\n",
    "                exp_rr[index] += cerr\n",
    "                accuracy[index] += cacc\n",
    "\n",
    "        mae_loss += F.l1_loss(y_hat, true_return_ratio).item()\n",
    "        rmse_returns_loss += F.mse_loss(y_hat, true_return_ratio).item()\n",
    "        mape += mean_absolute_percentage_error(y_hat, true_return_ratio)\n",
    "\n",
    "        move_pred = torch.where(y_hat >= 1, 1, 0)\n",
    "        move_loss += (move_pred.int() == move_target).float().mean()\n",
    "\n",
    "        mini = min(mini, y_hat.min().item())\n",
    "        maxi = max(maxi, y_hat.max().item())  \n",
    "        \n",
    "        epoch_loss += float(loss)\n",
    "\n",
    "    epoch_loss /= len(loader)\n",
    "    rmse_returns_loss /= len(loader)\n",
    "    move_loss  /= len(loader)\n",
    "    mape /= len(loader)\n",
    "    rr /= len(loader) \n",
    "    true_rr /= len(loader)\n",
    "    exp_rr /= len(loader)\n",
    "    ran_rr /= len(loader)\n",
    "    accuracy /= len(loader)\n",
    "    mae_loss /= len(loader)\n",
    "\n",
    "    print(\"[{0}] Movement Prediction Accuracy: {1}, MAPE: {2}\".format(desc, move_loss.item(), mape.item()))\n",
    "    print(\"[{0}] Range of predictions min: {1} max: {2}\".format(desc, mini, maxi))\n",
    "    print(\"[{0}] Epoch: {1} MSE: {2} RMSE: {3} Loss: {4} MAE: {5}\".format(desc, ep+1, rmse_returns_loss, rmse_returns_loss ** (1/2), epoch_loss, mae_loss))\n",
    "    \n",
    "    for index, k in enumerate(top_k_choice):\n",
    "        print(\"[{0}] Top {5} Return Ratio: {1} True Return Ratio: {2} Expected Return Ratio: {3} Random Return Ratio: {4} Accuracy: {6}\".format(desc, rr[index], true_rr[index], exp_rr[index], ran_rr[index], k, accuracy[index]))\n",
    "    \n",
    "    log = {'MSE': epoch_loss, 'RMSE': epoch_loss ** (1/2), \"MOVEMENT ACC\": move_loss, \"MAPE\": mape}\n",
    "    \n",
    "    if LOG:\n",
    "        wandb.log(log)\n",
    "    #    wandb.watch(model)\n",
    "\n",
    "    # matplotlib plot\n",
    "    #mpl.rcParams['figure.dpi']= 300\n",
    "    #plt.plot(np.array(yb_store).reshape(-1, 83), c='r')\n",
    "    #plt.plot(np.array(yhat_store).reshape(-1, 83), c='b')\n",
    "    #plt.show()\n",
    "\n",
    "    return epoch_loss, rr, true_rr, exp_rr, ran_rr, move_loss, mape, accuracy, mae_loss\n",
    "\n",
    "for tau in tau_choices:\n",
    "    tau_pos = tau_positions.index(tau)\n",
    "\n",
    "    print(\"Tau: \", tau, \"Tau Position: \", tau_pos)\n",
    "\n",
    "    # ----------- Batching the data -----------\n",
    "    def collate_fn(instn):\n",
    "        \n",
    "        # df: shape: BS x Companies x W+1 x 5 (5 is the number of features)\n",
    "        df = torch.Tensor(np.array([np.array([x[0] for x in ins]) for ins in instn])).unsqueeze(dim=3)\n",
    "        \n",
    "        for i in range(1, 5):\n",
    "            df1 = torch.Tensor(np.array([np.array([x[0] for x in ins]) for ins in instn])).unsqueeze(dim=3)\n",
    "            df = torch.cat((df, df1), dim=3)\n",
    "\n",
    "        company = torch.Tensor(np.array([np.array([x[5] for x in ins]) for ins in instn])).long()\n",
    "\n",
    "        # Shape: BS x Companies\n",
    "        target = torch.Tensor(np.array([np.array([x[7][tau_pos] for x in ins]) for ins in instn])).squeeze(dim=2)\n",
    "\n",
    "        # Shape: Companies x 1\n",
    "        scale = torch.Tensor(np.array([np.array([x[8] for x in ins]) for ins in instn]))\n",
    "\n",
    "        # Shape: BS x Companies\n",
    "        movement = target >= 1\n",
    "        return (df, company, target, scale, movement.int())\n",
    "\n",
    "\n",
    "    start_time = 0\n",
    "    test_mean_rr, test_mean_trr, test_mean_err, test_mean_rrr = torch.zeros(4).to(device), torch.zeros(4).to(device), torch.zeros(4).to(device), torch.zeros(4).to(device)\n",
    "    test_mean_move, test_mean_mape, test_mean_mae = 0, 0, 0\n",
    "\n",
    "    test_mean_acc = torch.zeros(4).to(device)\n",
    "    print(len(dataset))\n",
    "    for phase in range(1, 26):\n",
    "        print(\"Phase: \", phase)\n",
    "        train_loader    = DataLoader(dataset[start_time:start_time+250], 10, shuffle=True, collate_fn=collate_fn, num_workers=1)\n",
    "        val_loader      = DataLoader(dataset[start_time+250:start_time+300], 1, shuffle=True, collate_fn=collate_fn)\n",
    "        test_loader     = DataLoader(dataset[start_time+300:start_time+400], 1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        start_time += 100\n",
    "\n",
    "        model  = Transformer_Ranking(W, T, D_MODEL, N_HEAD, ENC_LAYERS, DEC_LAYERS, D_FF, DROPOUT, USE_POS_ENCODING, USE_GRAPH, HYPER_GRAPH, USE_KG, num_nodes)\n",
    "        #print(model)\n",
    "        model.to(device)\n",
    "\n",
    "        opt_c = torch.optim.Adam(model.parameters(), lr = LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4)\n",
    "\n",
    "        prev_val_loss = float(\"infinity\")\n",
    "        for ep in range(MAX_EPOCH):\n",
    "            print(\"Epoch: \" + str(ep+1))\n",
    "            \n",
    "            model.train()\n",
    "            train_epoch_loss, rr, trr, err, rrr, move, mape, accuracy, mae = predict(train_loader, \"TRAINING\")\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_epoch_loss, rr, trr, err, rrr, move, mape, accuracy, mae  = predict(val_loader, \"VALIDATION\")\n",
    "\n",
    "            #plot(val_loader)\n",
    "\n",
    "            if (ep > MAX_EPOCH//2 or ep > 10) and prev_val_loss > val_epoch_loss:\n",
    "                print(\"Saving Model\")\n",
    "                torch.save(model.state_dict(), \"models/saved_models/best_model_\"+INDEX+str(W)+\"_\"+str(T)+\"_\"+str(RUN)+\".pt\")\n",
    "                prev_val_loss = val_epoch_loss\n",
    "\n",
    "        model.load_state_dict(torch.load(\"models/saved_models/best_model_\"+INDEX+str(W)+\"_\"+str(T)+\"_\"+str(RUN)+\".pt\"))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_epoch_loss, rr, trr, err, rrr, move, mape, accuracy, mae = predict(test_loader, \"TESTING\")\n",
    "            test_mean_rr += rr\n",
    "            test_mean_trr += trr\n",
    "            test_mean_err += err\n",
    "            test_mean_rrr += rrr\n",
    "            test_mean_acc += accuracy\n",
    "            test_mean_move += float(move)\n",
    "            test_mean_mape += float(mape)\n",
    "            test_mean_mae += float(mae)\n",
    "            for index, k in enumerate(top_k_choice):\n",
    "                print(\"[Mean - {0}] Top {5} Return Ratio: {1} True Return Ratio: {2} Expected Return Ratio: {3} Random Return Ratio: {4} Accuracy: {6}\".format(\"TESTING\", test_mean_rr[index]/phase, test_mean_trr[index]/phase, test_mean_err[index]/phase, test_mean_rrr[index]/phase, k, test_mean_acc[index]/phase))\n",
    "            print(\"[Mean - {0}] Movement Accuracy: {1} Mean MAPE: {2} Mean MAE: {3}\".format(\"TESTING\", test_mean_move/phase, test_mean_mape/phase, test_mean_mae/phase))\n",
    "        if LOG:\n",
    "            wandb.save('model.py')\n",
    "\n",
    "    phase = 20\n",
    "    print(\"Tau: \", tau)\n",
    "    for index, k in enumerate(top_k_choice):\n",
    "        print(\"[Mean - {0}] Top {5} {1} {2} {3} {4} {6}\".format(\"TESTING\", test_mean_rr[index]/phase, test_mean_trr[index]/phase, test_mean_err[index]/phase, test_mean_rrr[index]/phase, k, test_mean_acc[index]/phase))\n",
    "    print(\"[Mean - {0}] {1} {2} {3}\".format(\"TESTING\", test_mean_move/phase, test_mean_mape/phase, test_mean_mae/phase))\n",
    "    gdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import math \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge, HypergraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "from torchkge.models.translation import TorusEModel\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # TODO: Check div term, correct or wrong\n",
    "        div_term = torch.exp(torch.arange(0, 2*d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)[:, 0::2]\n",
    "        #print(pe.shape, position.shape, div_term.shape)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:, 1::2]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "        # return x\n",
    "\n",
    "# ----------- Model -----------\n",
    "class Transformer_Ranking(nn.Module):\n",
    "    \n",
    "    def __init__(self, W, T, D_MODEL, N_HEAD, ENC_LAYERS, DEC_LAYERS, D_FF, DROPOUT, USE_POS_ENCODING = False, USE_GRAPH = False, HYPER_GRAPH = True, USE_KG = True, NUM_NODES = 87):\n",
    "        super().__init__()\n",
    "\n",
    "        SEC_EMB, n = 5, 1 # 1 For LSTM Embedding\n",
    "        if USE_GRAPH:\n",
    "            n += 1\n",
    "        if USE_KG:\n",
    "            n += 1\n",
    "\n",
    "        self.embeddings = nn.Embedding(105, 10)\n",
    "\n",
    "        self.pos_enc_x = PositionalEncoding(d_model=D_MODEL, dropout=DROPOUT, max_len=W)\n",
    "        self.pos_enc_y = PositionalEncoding(d_model=D_MODEL, dropout=DROPOUT, max_len=T)\n",
    "\n",
    "        self.lstm_encoder = nn.LSTM(input_size = D_MODEL, hidden_size = D_MODEL, num_layers = ENC_LAYERS, batch_first = True, bidirectional = False)\n",
    "\n",
    "        #encoder_layer = nn.TransformerEncoderLayer(d_model=D_MODEL, nhead=N_HEAD, dim_feedforward=D_FF, batch_first=True )\n",
    "        #self.transformer_encoder_first = nn.TransformerEncoder(encoder_layer, num_layers=ENC_LAYERS)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=D_MODEL, nhead=N_HEAD, dim_feedforward=D_FF, batch_first=True )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=ENC_LAYERS)\n",
    "\n",
    "        self.fc1 = nn.Linear(5, 5)\n",
    "        self.fc2 = nn.Linear(1, D_MODEL)\n",
    "        self.pred = nn.Linear(D_MODEL+(SEC_EMB*n), 1)\n",
    "        self.pred2 = nn.Linear(5, 1)\n",
    "\n",
    "        self.hold_pred = nn.Linear(D_MODEL+(SEC_EMB*n), 1)\n",
    "\n",
    "        self.is_pos = USE_POS_ENCODING\n",
    "        self.time_steps = T\n",
    "\n",
    "        \n",
    "\n",
    "        self.use_graph = USE_GRAPH\n",
    "        self.is_hyper_graph = HYPER_GRAPH\n",
    "        if self.use_graph:\n",
    "            if self.is_hyper_graph:\n",
    "                self.graph_model = Sequential('x, hyperedge_index', [\n",
    "                        #(Dropout(p=0.5), 'x -> x'),\n",
    "                        (HypergraphConv(8, 32, dropout=0.1), 'x, hyperedge_index -> x1'),\n",
    "                        nn.LeakyReLU(inplace=True),\n",
    "                        (HypergraphConv(32, 32, dropout=0.1), 'x1, hyperedge_index -> x2'),\n",
    "                        nn.LeakyReLU(inplace=True),\n",
    "                        \n",
    "                        #(lambda x1, x2: [x1, x2], 'x1, x2 -> xs'),\n",
    "                        #(JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -> x'),\n",
    "                        #(global_mean_pool, 'x, batch -> x'),\n",
    "                        nn.Linear(32, SEC_EMB),\n",
    "                    ])\n",
    "            else:\n",
    "                self.graph_model = Sequential('x, edge_index, batch', [\n",
    "                            #(Dropout(p=0.5), 'x -> x'),\n",
    "                            (GCNConv(8, 32), 'x, edge_index -> x1'),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            (GCNConv(32, 64), 'x1, edge_index -> x2'),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            #(lambda x1, x2: [x1, x2], 'x1, x2 -> xs'),\n",
    "                            #(JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -> x'),\n",
    "                            #(global_mean_pool, 'x, batch -> x'),\n",
    "                            nn.Linear(64, SEC_EMB),\n",
    "                        ])\n",
    "                \n",
    "        self.use_kg = USE_KG\n",
    "        if self.use_kg:\n",
    "            self.relation_kge = TorusEModel(n_entities= NUM_NODES, n_relations = 57, emb_dim = SEC_EMB, dissimilarity_type='torus_L2')\n",
    "        self.num_nodes = NUM_NODES\n",
    "\n",
    "    def forward(self, xb, yb=None, graph=None, kg=None):\n",
    "        if self.is_pos:\n",
    "            xb = self.pos_enc_x(xb)\n",
    "            yb = self.pos_enc_y(yb)\n",
    "        #yb = torch.cat((yb, emb2), dim=2)\n",
    "\n",
    "        # # Experiment 1\n",
    "        xb = xb.reshape(-1, xb.shape[2], xb.shape[3])\n",
    "        x, y = self.lstm_encoder(xb)\n",
    "        xb = y[0]\n",
    "        \n",
    "        xb = xb.view(-1, self.num_nodes, xb.shape[2])\n",
    "        # # Experiment 2\n",
    "        #W,F = xb.shape[1], xb.shape[2]\n",
    "        #xb = xb.unsqueeze(dim=0).view(1, -1, W*F)\n",
    "\n",
    "        # # Experiment 3\n",
    "        #x = self.transformer_encoder_first(xb).mean(dim=1)\n",
    "        #xb = x.unsqueeze(dim=0)\n",
    "        \n",
    "        x = self.transformer_encoder(xb)               # x: [B, C, W*F]\n",
    "        #x = torch.cat((x, emb2), dim=2)\n",
    "\n",
    "        if self.use_graph and self.is_hyper_graph:\n",
    "            g_emb = self.graph_model(graph['x'], graph['hyperedge_index']).unsqueeze(dim=0)\n",
    "            g_emb = g_emb.repeat(x.shape[0], 1, 1)\n",
    "            x = torch.cat((x, g_emb), dim=2)\n",
    "        elif self.use_graph and not self.is_hyper_graph:\n",
    "            g_emb = self.graph_model(graph['x'], graph['edge_list'], graph['batch'])\n",
    "            #g_emb = g_emb.repeat(1, self.time_steps, 1)        \n",
    "            x = torch.cat((x, g_emb), dim=1)\n",
    "        \n",
    "        kg_loss = torch.zeros(1)\n",
    "        if self.use_kg:\n",
    "            kg_loss = self.relation_kge.scoring_function(kg[0], kg[2], kg[1])\n",
    "            kg_emb, rel_emb = self.relation_kge.get_embeddings()\n",
    "            kg_emb = kg_emb.unsqueeze(dim=0)\n",
    "            x = torch.cat((x, kg_emb), dim=2)\n",
    "\n",
    "            self.relation_kge.normalize_parameters()\n",
    "\n",
    "        x = torch.cat((x, xb), dim=2)\n",
    "\n",
    "        price_pred = self.pred(x)\n",
    "        #price_pred = F.leaky_relu(price_pred)\n",
    "        #price_pred = F.relu(price_pred)\n",
    "\n",
    "        hold_pred = self.hold_pred(x.mean(dim=1)).squeeze(dim=0)\n",
    "            # x = F.relu(x)\n",
    "            # x = self.pred2(x)\n",
    "        return price_pred, kg_loss, hold_pred\n",
    "\n",
    "\n",
    "# ----------- Model -----------\n",
    "class Saturation(nn.Module):\n",
    "    \n",
    "    def __init__(self, W, T, D_MODEL, N_HEAD, ENC_LAYERS, DEC_LAYERS, D_FF, DROPOUT, USE_POS_ENCODING = False, USE_GRAPH = False, HYPER_GRAPH = True, USE_KG = True, NUM_NODES = 87):\n",
    "        super().__init__()\n",
    "\n",
    "        SEC_EMB, n = 5, 0 # 1 For LSTM Embedding\n",
    "        if USE_GRAPH:\n",
    "            n += 1\n",
    "        if USE_KG:\n",
    "            n += 1\n",
    "\n",
    "        self.lstm_encoder = nn.Linear(D_MODEL, 1)\n",
    "        self.transformer_encoder = nn.Linear(W, D_MODEL)\n",
    "\n",
    "        self.use_graph = USE_GRAPH\n",
    "        self.is_hyper_graph = HYPER_GRAPH\n",
    "        if self.use_graph:\n",
    "            if self.is_hyper_graph:\n",
    "                self.graph_model = Sequential('x, hyperedge_index', [\n",
    "                        #(Dropout(p=0.5), 'x -> x'),\n",
    "                        (HypergraphConv(8, 32, dropout=0.1), 'x, hyperedge_index -> x1'),\n",
    "                        nn.LeakyReLU(inplace=True),\n",
    "                        (HypergraphConv(32, 32, dropout=0.1), 'x1, hyperedge_index -> x2'),\n",
    "                        nn.LeakyReLU(inplace=True),\n",
    "                        \n",
    "                        #(lambda x1, x2: [x1, x2], 'x1, x2 -> xs'),\n",
    "                        #(JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -> x'),\n",
    "                        #(global_mean_pool, 'x, batch -> x'),\n",
    "                        nn.Linear(32, SEC_EMB),\n",
    "                    ])\n",
    "            else:\n",
    "                self.graph_model = Sequential('x, edge_index, batch', [\n",
    "                            #(Dropout(p=0.5), 'x -> x'),\n",
    "                            (GCNConv(8, 32), 'x, edge_index -> x1'),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            (GCNConv(32, 64), 'x1, edge_index -> x2'),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            #(lambda x1, x2: [x1, x2], 'x1, x2 -> xs'),\n",
    "                            #(JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -> x'),\n",
    "                            #(global_mean_pool, 'x, batch -> x'),\n",
    "                            nn.Linear(64, SEC_EMB),\n",
    "                        ])\n",
    "\n",
    "        self.pred = nn.Linear(D_MODEL, 1)\n",
    "\n",
    "    def forward(self, xb, yb=None, graph=None, kg=None):\n",
    "        x = self.lstm_encoder(xb).squeeze()\n",
    "        x = self.transformer_encoder(x)               # x: [B, C, W*F]\n",
    "\n",
    "        if self.use_graph and self.is_hyper_graph:\n",
    "            g_emb = self.graph_model(graph['x'], graph['hyperedge_index'])\n",
    "            #g_emb = g_emb.repeat(1, self.time_steps, 1)\n",
    "            #x = torch.cat((x, g_emb), dim=1)\n",
    "        elif self.use_graph and not self.is_hyper_graph:\n",
    "            g_emb = self.graph_model(graph['x'], graph['edge_list'], graph['batch'])\n",
    "            #g_emb = g_emb.repeat(1, self.time_steps, 1)        \n",
    "            #x = torch.cat((x, g_emb), dim=1)\n",
    "\n",
    "        price_pred = self.pred(x)\n",
    "        return price_pred, 0, 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
