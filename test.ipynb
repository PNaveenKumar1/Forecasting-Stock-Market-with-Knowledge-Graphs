{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = [\"nasdaq100\", \"sp500\", \"nifty500\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os, sys\n",
    "import pickle\n",
    "import math\n",
    "from queue import PriorityQueue\n",
    "from tkinter import N\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import pickle\n",
    "\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "\n",
    "from sklearn.metrics import accuracy_score, ndcg_score\n",
    "#from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from utils import (mean_absolute_percentage_error,\n",
    "                   load_or_create_dataset_graph,\n",
    "                   mean_square_error, root_mean_square_error)\n",
    "\n",
    "# from models.model_sai import Trans\n",
    "\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "#import tensorflow_ranking as tfr\n",
    "\n",
    "# from pytorchltr.loss import LambdaNDCGLoss1, LambdaNDCGLoss2\n",
    "from torchmetrics.functional import retrieval_normalized_dcg\n",
    "\n",
    "from random import randint\n",
    "import wandb\n",
    "GPU = 3\n",
    "LR = 0.0006\n",
    "BS = 128\n",
    "W = 20\n",
    "T = 20\n",
    "LOG = False\n",
    "D_MODEL = 20\n",
    "N_HEAD  = 5\n",
    "DROPOUT = 0.1\n",
    "D_FF    = 1024\n",
    "ENC_LAYERS = 1\n",
    "DEC_LAYERS = 1\n",
    "MAX_EPOCH = 10\n",
    "USE_POS_ENCODING = False\n",
    "USE_GRAPH = [True, False, 'hgat', 'rgat', 'gcn'][2]\n",
    "HYPER_GRAPH = [True, False][1]\n",
    "USE_RELATION_GRAPH = ['gcn', 'hypergraph', 'with_sector', False][3]\n",
    "USE_KG = [True, False][1]\n",
    "PREDICTION_PROBLEM = 'value'\n",
    "RUN = randint(1, 100000)\n",
    "PLOT = False\n",
    "MODEL_TYPE = '' #'random'\n",
    "ENCODER_LAYER = ['gru', 'transf', 'lstm'][2]\n",
    "\n",
    "tau_choices = [5]\n",
    "tau_positions = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File exists: Loading Dataset ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PREDICTION_PROBLEM = 'value'\n",
    "save_path = \"data/pickle/\"+INDEX+\"/full_graph_data_correct-P25-W\"+str(20)+\"-T\"+str(20)+\"_\"+str(PREDICTION_PROBLEM)+\".pkl\"\n",
    "\n",
    "dataset, company_to_id, graph, hyper_data = load_or_create_dataset_graph(INDEX=\"nasdaq100\", W=20, T=20, save_path=save_path, problem=PREDICTION_PROBLEM, fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(company_to_id.keys())\n",
    "node_type = torch.load('./kg/tkg_create/node_tensor_usa.pt')\n",
    "config = {\n",
    "            'entity_total': 6500,\n",
    "            'relation_total': 57,\n",
    "            'L1_flag': False,\n",
    "            'node_type': node_type,\n",
    "            'num_node_type': 14\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rank_loss(prediction, ground_truth):\n",
    "    all_one = torch.ones(prediction.shape[0], 1, dtype=torch.float32).to(device)\n",
    "    prediction = prediction.unsqueeze(dim=1)\n",
    "    ground_truth = ground_truth.unsqueeze(dim=1)\n",
    "    #print(prediction.shape, ground_truth.shape, base_price.shape)\n",
    "    return_ratio = prediction \n",
    "    true_return_ratio = ground_truth - 1\n",
    "\n",
    "    pre_pw_dif = torch.sub(\n",
    "        return_ratio @ all_one.t(),                  # C x C\n",
    "        all_one @ return_ratio.t()                   # C x C\n",
    "    )\n",
    "    gt_pw_dif = torch.sub(\n",
    "        all_one @ true_return_ratio.t(),\n",
    "        true_return_ratio @ all_one.t()\n",
    "    )\n",
    "\n",
    "    rank_loss = torch.mean(\n",
    "        F.relu(-1*pre_pw_dif * gt_pw_dif )\n",
    "    )\n",
    "   \n",
    "    return rank_loss \n",
    "\n",
    "def evaluate(prediction, ground_truth, bestret, worstret, K):\n",
    "    return_ratio = prediction - 1\n",
    "    true_return_ratio = ground_truth - 1\n",
    "    bestret = bestret - 1\n",
    "    worstret = worstret - 1\n",
    "\n",
    "    #print(\"True top k: \", torch.topk(true_return_ratio.squeeze(), k=3, dim=0))\n",
    "    #print(\"Predicted top k: \", torch.topk(return_ratio.squeeze(), k=3, dim=0))\n",
    "\n",
    "    target_obtained_return_ratio = torch.topk(true_return_ratio, k=K, dim=0)[0].mean()\n",
    "\n",
    "    #random = torch.randint(0, prediction.shape[0]-1, (K,))\n",
    "    #random_return_ratio = true_return_ratio[random].mean()\n",
    "    \n",
    "    topk_predicted = torch.topk(return_ratio, k=K, dim=0)[1]\n",
    "\n",
    "    #global MODEL_TYPE\n",
    "    #if MODEL_TYPE == 'random':\n",
    "    #    topk_predicted = random\n",
    "\n",
    "    obtained_return_ratio = true_return_ratio[topk_predicted].mean()\n",
    "    best_return_ratio = bestret[topk_predicted].mean()\n",
    "    worst_return_ratio = worstret[topk_predicted].mean()\n",
    "    #return_ratio = -1*return_ratio\n",
    "    #obtained_return_ratio += true_return_ratio[torch.topk(return_ratio.squeeze(), k=K, dim=0)[1]].mean()\n",
    "    #obtained_return_ratio /= 2\n",
    "\n",
    "    a_cat_b, counts = torch.cat([torch.topk(return_ratio.squeeze(), k=K, dim=0)[1], torch.topk(true_return_ratio.squeeze(), k=K, dim=0)[1]]).unique(return_counts=True)\n",
    "    accuracy = a_cat_b[torch.where(counts.gt(1))].shape[0] / K\n",
    "\n",
    "    return obtained_return_ratio, target_obtained_return_ratio, accuracy, best_return_ratio, worst_return_ratio\n",
    "\n",
    "\n",
    "top_k_choice = [1, 5]\n",
    "\n",
    "def calculate_ndcg(predict, true, k):\n",
    "    tt = torch.topk(true, k, dim=0)[1]\n",
    "    rel_score = torch.arange(k, 0, -1).to(device)\n",
    "    true_rel = torch.zeros_like(predict).long()\n",
    "    true_rel[tt] = rel_score\n",
    "    return retrieval_normalized_dcg(predict, true_rel)\n",
    "\n",
    "def approxNDCGLoss(y_pred, y_true, eps=1e-10, padded_value_indicator=-1, alpha=1.):\n",
    "    \"\"\"\n",
    "    Loss based on approximate NDCG introduced in \"A General Approximation Framework for Direct Optimization of\n",
    "    Information Retrieval Measures\". Please note that this method does not implement any kind of truncation.\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :param eps: epsilon value, used for numerical stability\n",
    "    :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
    "    :param alpha: score difference weight used in the sigmoid function\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    # shuffle for randomised tie resolution\n",
    "    random_indices = torch.randperm(y_pred.shape[-1])\n",
    "    y_pred_shuffled = y_pred.unsqueeze(dim=0)[:, random_indices]\n",
    "    y_true_shuffled = y_true.unsqueeze(dim=0)[:, random_indices]\n",
    "\n",
    "    y_true_sorted, indices = y_true_shuffled.sort(descending=True, dim=-1)\n",
    "\n",
    "    mask = y_true_sorted == padded_value_indicator\n",
    "\n",
    "    preds_sorted_by_true = torch.gather(y_pred_shuffled, dim=1, index=indices)\n",
    "    preds_sorted_by_true[mask] = float(\"-inf\")\n",
    "\n",
    "    max_pred_values, _ = preds_sorted_by_true.max(dim=1, keepdim=True)\n",
    "\n",
    "    preds_sorted_by_true_minus_max = preds_sorted_by_true - max_pred_values\n",
    "\n",
    "    cumsums = torch.cumsum(preds_sorted_by_true_minus_max.exp().flip(dims=[1]), dim=1).flip(dims=[1])\n",
    "\n",
    "    observation_loss = torch.log(cumsums + eps) - preds_sorted_by_true_minus_max\n",
    "\n",
    "    observation_loss[mask] = 0.0\n",
    "\n",
    "    return torch.mean(torch.sum(observation_loss, dim=1))\n",
    "\n",
    "def approx_rank(logits):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        logits (_type_): A `Tensor` with shape [batch_size, list_size]. Each value is the\n",
    "      ranking score of the corresponding item.\n",
    "\n",
    "    Returns:\n",
    "        _type_: A `Tensor` of ranks with the same shape as logits.\n",
    "    \"\"\"\n",
    "    list_size = logits.shape[1]\n",
    "    x = logits.unsqueeze(2).repeat(1, 1, list_size)\n",
    "    y = logits.unsqueeze(1).repeat(1, list_size, 1)\n",
    "    rank = torch.sigmoid(x - y)\n",
    "    rank = torch.sum(rank, dim=-1) #+ 0.5\n",
    "    return rank\n",
    "\n",
    "def approx_ndcg_loss(logits, labels):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        logits (_type_): A `Tensor` with shape [batch_size, list_size]. Each value is the\n",
    "      ranking score of the corresponding item.\n",
    "        labels (_type_): A `Tensor` with shape [batch_size, list_size]. Each value is the\n",
    "      relevance label of the corresponding item.\n",
    "\n",
    "    Returns:\n",
    "        _type_: A `Tensor` of ndcg loss with shape [batch_size].\n",
    "    \"\"\"\n",
    "    rank = approx_rank(logits)\n",
    "    #print(\"logits\", torch.topk(logits, k=5, dim=-1), torch.topk(rank, k=5, dim=-1))\n",
    "    return - retrieval_normalized_dcg(rank, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_file_name = './kg/tkg_create/temporal_kg.pkl'\n",
    "# if INDEX == 'nifty500':\n",
    "#     kg_file_name = './kg/tkg_create/temporal_kg_nifty.pkl'\n",
    "with open(kg_file_name, 'rb') as f:\n",
    "    pkl_file = pickle.load(f)\n",
    "\n",
    "    # if \"nasdaq\" in INDEX:\n",
    "    kg_map = pkl_file['nasdaq_map']\n",
    "    # elif \"sp\" in INDEX:\n",
    "    #     kg_map = pkl_file['sp_map']\n",
    "    # elif \"nifty\" in INDEX:\n",
    "    #     kg_map = pkl_file['nifty_map']\n",
    "#print(kg_map)\n",
    "\n",
    "# if USE_KG:\n",
    "    #kg_file_name = './kg/profile_and_relationship/wikidata/'+INDEX+'_relations_kg.pkl'\n",
    "\n",
    "    relation_kg = None\n",
    "    \n",
    "    \"\"\"\n",
    "    kg_file_name = './kg/profile_and_relationship/wikidata/entire_kg.pkl'\n",
    "    with open(kg_file_name, 'rb') as f:\n",
    "        pkl_file = pickle.load(f)\n",
    "        relation_kg = pkl_file['kg']\n",
    "        if INDEX == 'nasdaq100':\n",
    "            kg_index = pkl_file['nasdaq_map']\n",
    "        else:\n",
    "            kg_index = pkl_file['sp_map']\n",
    "    \n",
    "    head, relation, tail = relation_kg[0].long(), relation_kg[1].long(), relation_kg[2].long()\n",
    "    print(head.max(), relation.max(), tail.max())\n",
    "    head, relation, tail, kg_index = head.to(device), relation.to(device), tail.to(device), kg_index.to(device)\n",
    "    relation_kg = (head, relation, tail, kg_index)\n",
    "    \"\"\"\n",
    "# else:\n",
    "#     relation_kg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     relation_graph \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)[key]\n\u001b[0;32m----> 7\u001b[0m relation_graph \u001b[38;5;241m=\u001b[39m relation_graph\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    297\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_init()\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    302\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:\"+str(5))\n",
    "INDEX = [\"nasdaq100\", \"sp500\", \"nifty500\"][1]\n",
    "key = INDEX\n",
    "file_path = './kg/profile_and_relationship/wikidata/relation_graph.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    relation_graph = pickle.load(f)[key]\n",
    "relation_graph = relation_graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_nodes_batch = torch.zeros(graph.x.shape[0]).to(device)\n",
    "graph = graph.to(device)\n",
    "graph_data = {\n",
    "    'x': graph.x,\n",
    "    'edge_list': graph.edge_index,\n",
    "    'batch': graph_nodes_batch\n",
    "}\n",
    "file_path = './kg/profile_and_relationship/wikidata/relation_graph.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    relation_graph = pickle.load(f)[key]\n",
    "relation_graph = relation_graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rr_list = [[], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau:  5 Tau Position:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data5/home/saihadnoor/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from models.model_trans_dec import Trans\n",
    "for tau in tau_choices:\n",
    "    tau_pos = tau_positions.index(tau)\n",
    "\n",
    "    print(\"Tau: \", tau, \"Tau Position: \", tau_pos)\n",
    "\n",
    "    start_time, train_begin = 0, 0\n",
    "    test_mean_rr, test_mean_trr, test_mean_err, test_mean_rrr = [[], [], []], [[], [], []], [[], [], []], [[], [], []]\n",
    "    test_mean_ndcg, test_mean_acc = [[], [], [], []], [[], [], []]\n",
    "    test_mean_brr, test_mean_wrr, test_mean_sharpe = torch.zeros(4).to(device), torch.zeros(4).to(device), [[], [], []]\n",
    "    def collate_fn(instn):\n",
    "        tkg = instn[0][1]\n",
    "        instn = instn[0][0]\n",
    "        df = torch.Tensor(np.array([x[0] for x in instn])).unsqueeze(dim=2)\n",
    "        for i in range(1, 5):\n",
    "            df1 = torch.Tensor(np.array([x[i] for x in instn])).unsqueeze(dim=2)\n",
    "            df = torch.cat((df, df1), dim=2)\n",
    "        min_val = df.min()\n",
    "        max_val = df.max()\n",
    "\n",
    "        # Normalize tensor to the range [-1, 1]\n",
    "        normalized_tensor = 2 * (df - min_val) / (max_val - min_val) - 1\n",
    "        target = torch.Tensor(np.array([x[7][tau_pos] for x in instn]))\n",
    "\n",
    "        best_case, worst_case = torch.Tensor(np.array([x[11][tau_pos+1] for x in instn])), torch.Tensor(np.array([x[10][tau_pos+1] for x in instn]))\n",
    "        best_case = best_case / torch.Tensor(np.array([x[10][0] for x in instn]))\n",
    "        worst_case = worst_case / torch.Tensor(np.array([x[11][0] for x in instn]))\n",
    "\n",
    "        return (normalized_tensor, target, tkg, best_case, worst_case)\n",
    "    train_loader    = DataLoader(dataset[train_begin:start_time+400], 1, shuffle=True, collate_fn=collate_fn, num_workers=1)\n",
    "    model  = Trans(W, T, D_MODEL, N_HEAD, ENC_LAYERS, DEC_LAYERS, D_FF, DROPOUT, USE_POS_ENCODING, USE_GRAPH, HYPER_GRAPH, USE_KG, num_nodes, config, \"transf\", USE_RELATION_GRAPH)\n",
    "    model.to(device)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement_values = []\n",
    "for xb, yb, tkg, bestret, worstret in train_loader:\n",
    "    head, relation, tail, ts = tkg\n",
    "    head, relation, tail, ts, kg_map = head.to(device), relation.to(device), tail.to(device), ts.to(device), kg_map.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([83, 20, 5])\n",
      "torch.Size([83, 5, 20])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "expecting key_padding_mask shape of (83, 5), but got torch.Size([336, 5])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m x_t \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_t\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 14\u001b[0m y_hat, kg_loss \u001b[38;5;241m=\u001b[39m model(xb, yb, graph_data, relation_kg, tkg, relation_graph)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(kg_loss)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Stock_Origin_Kawin/Phase-Stock-KG/models/model_trans_dec.py:165\u001b[0m, in \u001b[0;36mTrans.forward\u001b[0;34m(self, xb, yb, graph, kg, tkg, relation_graph)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe tensor contains NaN values.\u001b[39m\u001b[38;5;124m\"\u001b[39m,contains_nan)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValues in the tensor are Nan.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_decoder_first(tgt\u001b[38;5;241m=\u001b[39mx_t, memory\u001b[38;5;241m=\u001b[39mx_enc, memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_mask)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# dec_out = self.transformer_decoder_first(tgt=x_t , memory= x_enc).mean(dim=1)\u001b[39;00m\n\u001b[1;32m    167\u001b[0m x \u001b[38;5;241m=\u001b[39m dec_out\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:460\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    457\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 460\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(output, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[1;32m    461\u001b[0m                  memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    462\u001b[0m                  tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    463\u001b[0m                  memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    464\u001b[0m                  tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal,\n\u001b[1;32m    465\u001b[0m                  memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:847\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    846\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[0;32m--> 847\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[1;32m    848\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:865\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mha_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[1;32m    864\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 865\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultihead_attn(x, mem, mem,\n\u001b[1;32m    866\u001b[0m                             attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m    867\u001b[0m                             key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    868\u001b[0m                             is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    869\u001b[0m                             need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1242\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[1;32m   1245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1246\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1247\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1248\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1249\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1250\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1251\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:5379\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5377\u001b[0m \u001b[38;5;66;03m# merge key padding and attention masks\u001b[39;00m\n\u001b[1;32m   5378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 5379\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m key_padding_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (bsz, src_len), \\\n\u001b[1;32m   5380\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting key_padding_mask shape of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39msrc_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey_padding_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5381\u001b[0m     key_padding_mask \u001b[38;5;241m=\u001b[39m key_padding_mask\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, src_len)\u001b[38;5;241m.\u001b[39m   \\\n\u001b[1;32m   5382\u001b[0m         expand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(bsz \u001b[38;5;241m*\u001b[39m num_heads, \u001b[38;5;241m1\u001b[39m, src_len)\n\u001b[1;32m   5383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: expecting key_padding_mask shape of (83, 5), but got torch.Size([336, 5])"
     ]
    }
   ],
   "source": [
    "for xb, yb, tkg, bestret, worstret in train_loader:\n",
    "    head, relation, tail, ts = tkg\n",
    "    head, relation, tail, ts, kg_map = head.to(device), relation.to(device), tail.to(device), ts.to(device), kg_map.to(device)\n",
    "\n",
    "    tkg = (head, relation, tail, ts, kg_map)\n",
    "\n",
    "    xb      = xb.to(device)     \n",
    "    yb      = yb.to(device) \n",
    "    bestret = bestret.to(device)\n",
    "    worstret = worstret.to(device)\n",
    "    print(xb.shape)\n",
    "    x_t = xb.transpose(1,2)\n",
    "    print(x_t.shape)\n",
    "    y_hat, kg_loss = model(xb, yb, graph_data, relation_kg, tkg, relation_graph)\n",
    "    print(kg_loss)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kg_file_name = './kg/tkg_create/temporal_kg.pkl'\n",
    "with open(kg_file_name, 'rb') as f:\n",
    "    pkl_file = pickle.load(f)\n",
    "    relation_kg = pkl_file['temporal_kg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau:  5 Tau Position:  0\n",
      "Phase:  1\n",
      "Trans(\n",
      "  (embeddings): Embedding(105, 10)\n",
      "  (transformer_decoder_first): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=20, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=20, bias=True)\n",
      "        (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_encoder_first): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=20, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=20, bias=True)\n",
      "        (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=20, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=20, bias=True)\n",
      "  (pred): Linear(in_features=5810, out_features=83, bias=True)\n",
      "  (pred2): Linear(in_features=830, out_features=83, bias=True)\n",
      "  (hold_pred): Linear(in_features=70, out_features=1, bias=True)\n",
      "  (conv1): HEATConv(20, 8, heads=4)\n",
      "  (conv2): HEATConv(32, 16, heads=4)\n",
      "  (lin): Linear(in_features=64, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=50, out_features=16, bias=True)\n",
      "  (rel_type_embeddings): Embedding(59, 16)\n",
      "  (month_embeddings): Embedding(13, 16, padding_idx=0)\n",
      "  (day_embeddings): Embedding(32, 16, padding_idx=0)\n",
      "  (hour_embeddings): Embedding(25, 16, padding_idx=0)\n",
      "  (minutes_embeddings): Embedding(61, 16, padding_idx=0)\n",
      "  (sec_embeddings): Embedding(61, 16, padding_idx=0)\n",
      "  (ent_embeddings): Embedding(6502, 20)\n",
      ")\n",
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m bestret \u001b[38;5;241m=\u001b[39m bestret\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    101\u001b[0m worstret \u001b[38;5;241m=\u001b[39m worstret\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 103\u001b[0m y_hat, kg_loss \u001b[38;5;241m=\u001b[39m model(xb, yb, graph_data, relation_kg, tkg, relation_graph)\n\u001b[1;32m    104\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m y_hat\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Stock_Origin_Kawin/Phase-Stock-KG/models/model_trans_dec.py:187\u001b[0m, in \u001b[0;36mTrans.forward\u001b[0;34m(self, xb, yb, graph, kg, tkg, relation_graph)\u001b[0m\n\u001b[1;32m    184\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ment_embeddings\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    185\u001b[0m weight[tkg[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ment_embeddings\u001b[38;5;241m.\u001b[39mweight[tkg[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()] \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 187\u001b[0m gx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(weight, edge, node_type, tkg[\u001b[38;5;241m1\u001b[39m], edge_attr)\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m#gx = self.bn1(gx)\u001b[39;00m\n\u001b[1;32m    189\u001b[0m gx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(gx, edge, node_type, tkg[\u001b[38;5;241m1\u001b[39m], edge_attr)\u001b[38;5;241m.\u001b[39mrelu()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/conv/heat_conv.py:104\u001b[0m, in \u001b[0;36mHEATConv.forward\u001b[0;34m(self, x, edge_index, node_type, edge_type, edge_attr)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, edge_index: Adj, node_type: Tensor,\n\u001b[1;32m    102\u001b[0m             edge_type: Tensor, edge_attr: OptTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 104\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhetero_lin(x, node_type)\n\u001b[1;32m    106\u001b[0m     edge_type_emb \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_type_emb(edge_type),\n\u001b[1;32m    107\u001b[0m                                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_slope)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_type_emb: Tensor, edge_attr: OptTensor)  # noqa\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/dense/linear.py:276\u001b[0m, in \u001b[0;36mHeteroLinear.forward\u001b[0;34m(self, x, type_vec)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m subset_out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(x[mask], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight[i]\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# The data type may have changed with mixed precision:\u001b[39;00m\n\u001b[1;32m    278\u001b[0m out[mask] \u001b[38;5;241m=\u001b[39m subset_out\u001b[38;5;241m.\u001b[39mto(out\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1682\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1684\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for tau in tau_choices:\n",
    "    tau_pos = tau_positions.index(tau)\n",
    "\n",
    "    print(\"Tau: \", tau, \"Tau Position: \", tau_pos)\n",
    "\n",
    "    # ----------- Batching the data -----------\n",
    "    def collate_fn(instn):\n",
    "        tkg = instn[0][1]\n",
    "        instn = instn[0][0]\n",
    "\n",
    "        # df: shape: Companies x W+1 x 5 (5 is the number of features)\n",
    "        df = torch.Tensor(np.array([x[0] for x in instn])).unsqueeze(dim=2)\n",
    "        #df = torch.Tensor(np.array([x[1] for x in instn])).unsqueeze(dim=2) - torch.Tensor(np.array([x[2] for x in instn])).unsqueeze(dim=2)\n",
    "        for i in range(1, 5):\n",
    "            df1 = torch.Tensor(np.array([x[i] for x in instn])).unsqueeze(dim=2)\n",
    "            df = torch.cat((df, df1), dim=2)\n",
    "\n",
    "        # Shape: Companies x 1\n",
    "        target = torch.Tensor(np.array([x[7][tau_pos] for x in instn]))\n",
    "\n",
    "        # Shape: Companies x 1\n",
    "        #movement = target >= 1\n",
    "\n",
    "        best_case, worst_case = torch.Tensor(np.array([x[11][tau_pos+1] for x in instn])), torch.Tensor(np.array([x[10][tau_pos+1] for x in instn]))\n",
    "        best_case = best_case / torch.Tensor(np.array([x[10][0] for x in instn]))\n",
    "        worst_case = worst_case / torch.Tensor(np.array([x[11][0] for x in instn]))\n",
    "\n",
    "        return (df, target, tkg, best_case, worst_case)\n",
    "\n",
    "\n",
    "    start_time, train_begin = 0, 0\n",
    "    test_mean_rr, test_mean_trr, test_mean_err, test_mean_rrr = [[], [], []], [[], [], []], [[], [], []], [[], [], []]\n",
    "    test_mean_ndcg, test_mean_acc = [[], [], [], []], [[], [], []]\n",
    "    test_mean_brr, test_mean_wrr, test_mean_sharpe = torch.zeros(4).to(device), torch.zeros(4).to(device), [[], [], []]\n",
    "\n",
    "    for phase in range(1, 22):\n",
    "        print(\"Phase: \", phase)\n",
    "\n",
    "\n",
    "        # with open(\"/data5/home/saihadnoor/Stock_Origin_Kawin/Phase-Stock-KG/pickle/\"+INDEX+\"_\"+str(phase)+\"_25phase.pkl\", \"rb\") as f:\n",
    "        #     # pickle.dump(dataset[train_begin:start_time+400], f)\n",
    "        #     dataset1 = pickle.load(f)\n",
    "        #     print(len(dataset1))\n",
    "        # train_loader    = DataLoader(dataset1, 1, shuffle=True, collate_fn=collate_fn, num_workers=1)\n",
    "        train_loader    = DataLoader(dataset[train_begin:start_time+400], 1, shuffle=True, collate_fn=collate_fn, num_workers=1)\n",
    "        # val_loader      = DataLoader(dataset[start_time+400:start_time+450], 1, shuffle=False, collate_fn=collate_fn)\n",
    "        # test_loader     = DataLoader(dataset[start_time+450:start_time+550], 1, shuffle=False, collate_fn=collate_fn)   \n",
    "        #print(len(dataset), len(dataset[start_time:start_time+1000]), len(dataset[start_time+1000:start_time+1100]), len(dataset[start_time+1100:start_time+1400]))\n",
    "        start_time += 100\n",
    "        if start_time >= 300:\n",
    "            train_begin += 100      \n",
    "\n",
    "        node_type = torch.load('./kg/tkg_create/node_tensor_usa.pt')\n",
    "        config = {\n",
    "            'entity_total': 6500,\n",
    "            'relation_total': 57,\n",
    "            'L1_flag': False,\n",
    "            'node_type': node_type,\n",
    "            'num_node_type': 14\n",
    "        }\n",
    "\n",
    "        if INDEX == 'nifty500':\n",
    "            node_type = torch.load('./kg/tkg_create/node_tensor_india.pt')\n",
    "            config = {\n",
    "                'entity_total': 1200,\n",
    "                'relation_total': 57,\n",
    "                'L1_flag': False,\n",
    "                'node_type': node_type,\n",
    "                'num_node_type': 16\n",
    "            }\n",
    "        model  = Trans(W, T, D_MODEL, N_HEAD, ENC_LAYERS, DEC_LAYERS, D_FF, DROPOUT, USE_POS_ENCODING, USE_GRAPH, HYPER_GRAPH, USE_KG, num_nodes, config, \"transf\", USE_RELATION_GRAPH)\n",
    "        if phase == 1:\n",
    "            print(model)\n",
    "        model.to(device)\n",
    "\n",
    "        #if phase > 1:\n",
    "        #    model.load_state_dict(torch.load(\"models/saved_models/best_model_\"+INDEX+str(W)+\"_\"+str(T)+\"_\"+str(RUN)+\".pt\"))\n",
    "        #nasdaq 1e-5, 4e-5\n",
    "        opt_c = torch.optim.AdamW(model.parameters(), lr = 2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "        opt_kg = torch.optim.Adam(model.parameters(), lr = 4e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "        #opt_c = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0, nesterov=True)\n",
    "\n",
    "        prev_val_loss, best_val_loss = float(\"infinity\"), float(\"infinity\")\n",
    "        val_loss_history = []\n",
    "        for ep in range(4):\n",
    "            print(\"Epoch: \" + str(ep+1))\n",
    "            model.train()\n",
    "            rr, true_rr, accuracy, best_rr, worst_rr = torch.zeros(4).to(device), torch.zeros(4).to(device), torch.zeros(4).to(device), torch.zeros(4).to(device), torch.zeros(4).to(device)\n",
    "            ndcg, sharpe_ratio = torch.zeros(4).to(device), torch.zeros(4).to(device)\n",
    "            sharpe = [[], [], []]\n",
    "            yb_store, yhat_store, yb_store2 = [], [], []\n",
    "            for xb, yb, tkg, bestret, worstret in train_loader:\n",
    "                head, relation, tail, ts = tkg\n",
    "                head, relation, tail, ts, kg_map = head.to(device), relation.to(device), tail.to(device), ts.to(device), kg_map.to(device)\n",
    "\n",
    "                tkg = (head, relation, tail, ts, kg_map)\n",
    "\n",
    "                xb      = xb.to(device)     \n",
    "                yb      = yb.to(device) \n",
    "                bestret = bestret.to(device)\n",
    "                worstret = worstret.to(device)\n",
    "\n",
    "                y_hat, kg_loss = model(xb, yb, graph_data, relation_kg, tkg, relation_graph)\n",
    "                y_hat = y_hat.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kg_file_name = './kg/tkg_create/temporal_kg.pkl'\n",
    "with open(kg_file_name, 'rb') as f:\n",
    "    pkl_file = pickle.load(f)\n",
    "    relation_kg = pkl_file['temporal_kg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370567\n"
     ]
    }
   ],
   "source": [
    "print(len(relation_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>tail</th>\n",
       "      <th>relation</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>expiry_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1007</td>\n",
       "      <td>2591</td>\n",
       "      <td>1</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5052</th>\n",
       "      <td>1340</td>\n",
       "      <td>4079</td>\n",
       "      <td>12</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>1232</td>\n",
       "      <td>4078</td>\n",
       "      <td>12</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5050</th>\n",
       "      <td>1232</td>\n",
       "      <td>4077</td>\n",
       "      <td>12</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5049</th>\n",
       "      <td>1232</td>\n",
       "      <td>4076</td>\n",
       "      <td>12</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      head  tail  relation                 timestamp                 expiry_ts\n",
       "0     1007  2591         1 1970-01-01 00:00:00+00:00 1970-01-01 00:00:00+00:00\n",
       "5052  1340  4079        12 1970-01-01 00:00:00+00:00 1970-01-01 00:00:00+00:00\n",
       "5051  1232  4078        12 1970-01-01 00:00:00+00:00 1970-01-01 00:00:00+00:00\n",
       "5050  1232  4077        12 1970-01-01 00:00:00+00:00 1970-01-01 00:00:00+00:00\n",
       "5049  1232  4076        12 1970-01-01 00:00:00+00:00 1970-01-01 00:00:00+00:00"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_kg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1970, 1970, 1970, 1970, 1970])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_kg.head()['timestamp'].dt.year.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkg = relation_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7569]) torch.Size([1, 7569])\n"
     ]
    }
   ],
   "source": [
    "a =tkg[0].unsqueeze(0)\n",
    "b= tkg[2].unsqueeze(0)\n",
    "print(a.shape,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1007, 1340, 1232,  ..., 1130, 1135,  845],\n",
      "        [2591, 4079, 4078,  ..., 1112, 1112,  600]], device='cuda:5')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7569])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge = torch.cat((a,b),dim=0)\n",
    "print(edge)\n",
    "edge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 7], device='cuda:5')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7569])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tkg[3][:, 1]\n",
    "print(c)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tkg[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
